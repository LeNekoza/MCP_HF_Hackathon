{
  "server_name": "127.0.0.1",
  "server_port": 7860,
  "share": false,
  "debug": true,
  "default_model": "gpt-3.5-turbo",
  "max_tokens": 1000,
  "temperature": 0.7,
  "app_title": "MCP HF Hackathon",
  "app_description": "Model Context Protocol Integration with Hugging Face",
  "models": {
    "available": [
      "gpt-3.5-turbo",
      "gpt-4",
      "claude-3-sonnet",
      "llama-2-7b",
      "mistral-7b"
    ],
    "huggingface": {
      "api_url": "https://api-inference.huggingface.co/models/",
      "cache_dir": "./data/models"
    }
  },
  "mcp": {
    "enabled": true,
    "version": "1.0",
    "protocol_features": [
      "context_management",
      "model_switching",
      "response_streaming"
    ]
  }
}
